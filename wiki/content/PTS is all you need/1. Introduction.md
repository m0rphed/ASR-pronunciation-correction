
Существующие работы были посвящены различным задачам, таким как:
- обнаружение неправильно произнесенных фонем [9],
- обнаружение лексических ошибок ударения [13]. 

Наибольшее внимание исследователи уделяют изучению различных моделей машинного обучения, таких как:
- байесовские сети [14, 15],
- методы глубокого обучения [9, 10], 
а также анализу различных представлений речевого сигнала, таких как 
- просодические (prosodic) признаки (длительность, энергия и высота тона) [16],
- цепстральные cepstral/спектральные признаки [9, 13, 17]. 

Несмотря на значительный прогресс, достигнутый в последние годы, существующие методы CAPT обнаруживают ошибки произношения с относительно низкой точностью - 60 % точности при 40-80 % запоминания [9-11]. 

Интуитивно понятно, что если бы у нас была генеративная модель, которая могла бы имитировать неправильно произнесенную речь и выдавать любое количество обучающих данных, то задача обнаружения ошибок произношения была бы намного проще. 

Вероятность ошибок произношения для всех слов в предложении 
можно представить следующим образом по правилу Байеса. 

$P(\text{ошибка}|\text{звуки}) = 1 - P(\text{слово}|\text{звуки})$ 

$P(\text{слово}|\text{звуки}) = \frac{P(\text{звуки}|\text{слово}) P(\text{слово})}{P(\text{звуки})}$

- $P(\text{ошибка}|\text{звуки})$ — это вероятность того, что слово было произнесено **неправильно**, исходя из услышанных звуков. 
- $P(\text{слово}|\text{звуки})$ — это вероятность того, что было произнесено **правильное** слово.
- $P(\text{звуки}|\text{слово})$ — вероятность того, что определённые звуки будут услышаны при правильном произношении слова. 
- $P(\text{слово})$ — вероятность появления самого слова (до учёта звуков), то есть насколько слово распространено. 
- $P(\text{звуки})$ — общая вероятность того, что такие звуки будут услышаны, независимо от правильного или ошибочного произношения.

Предложенный метод превосходит современную модель [9] в обнаружении ошибок произношения по метрике AUC на 41 % с 0,528 до 0,749 на корпусе польских дикторов L2 GUT Isle Corpus.

Чтобы реализовать новую постановку задачи, мы предлагаем три 
инновационные методики, основанные на преобразовании фонем в фонемы (P2P), текста в речь (T2S) и речи в речь (S2S), для генерации правильно произнесенной и неправильно произнесенной синтаксической речи. 

Мы показываем, что эти методы не только повышают точность трех моделей машинного обучения для обнаружения ошибок произношения, но и помогают установить новый уровень техники в этой области. 

Эффективность этих методов оценивается в двух задачах: 
- обнаружение неправильно произнесенных слов (замена, добавление, удаление фонем или произнесение неизвестного звука речи)
- обнаружение лексических ошибок ударения

Результаты, представленные в данном исследовании, являются кульминацией нашей недавней работы по генерации речи в задаче обнаружения ошибок произношения [11, 22, 23], включая новую технику S2S.

Вкратце, вклад данной статьи заключается в следующем:
• Предлагается новая парадигма для автоматического обнаружения ошибок произношения, переформулирующая проблему как задачу генерации синтетической речи.
• В контексте обнаружения ошибок произношения представлен единый вероятностный взгляд на методы P2P, T2S и S2S.
• Предложен новый метод S2S для генерации синтетической речи, который превосходит современную модель [9] в обнаружении ошибок произношения.
• Описаны комплексные эксперименты, демонстрирующие эффективность генерации речи в задачах определения произношения и лексических ошибок ударения.

---
[9] W.K. Leung, X. Liu, and H. Meng, CNN-RNN-CTC based end-to-end mispronunciation detection and diagnosis, in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 8132–8136.
[10] Z. Zhang, Y. Wang, and J. Yang, Text-conditioned transformer for automatic pronunci-
ation error detection, Speech Communication 130 (2021), pp. 55–63.
[11] D. Korzekwa, J. Lorenzo-Trueba, S. Zaporowski, S. Calamaro, T. Drugman, and B.
Kostek, Mispronunciation Detection in Non-Native (L2) English with Uncertainty Mod-
eling, in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and
Signal Processing (ICASSP). IEEE, 2021, pp. 7738–7742.
[12] R. Ai, Automatic pronunciation error detection and feedback generation for call applica-
tions, in International Conference on Learning and Collaboration Technologies. Springer,
2015, pp. 175–186.
[13] L. Ferrer, H. Bratt, C. Richey, H. Franco, V. Abrash, and K. Precoda, Classiﬁcation of
lexical stress using spectral and prosodic features for computer-assisted language learning
systems, Speech Communication 69 (2015), pp. 31–45.
[14] S.M. Witt and S.J. Young, Phone-level pronunciation scoring and assessment for inter-
active language learning, Speech communication 30 (2000), pp. 95–108.
[15] H. Li, S. Huang, S. Wang, and B. Xu, Context-Dependent Duration Modeling with Back-
oﬀ Strategy and Look-Up Tables for Pronunciation Assessment and Mispronunciation
Detection, in INTERSPEECH 2011, 12th Annual Conference of the International Speech
Communication Association, Florence, Italy, August 27-31, 2011. ISCA, 2011, pp. 1133–1136.
[16] J.Y. Chen and L. Wang, Automatic lexical stress detection for Chinese learners’ of En-
glish, in 2010 7th Intl. Symposium on Chinese Spoken Language Processing. IEEE, 2010,
pp. 407–411.
[17] M.A. Shahin, J. Epps, and B. Ahmed, Automatic Classiﬁcation of Lexical Stress in English
and Arabic Languages Using Deep Learning., in INTERSPEECH. 2016, pp. 175–179.
[18] C.M. Bishop, Pattern recognition, Machine learning 128 (2006).
[19] G. Huybrechts, T. Merritt, G. Comini, B. Perz, R. Shah, and J. Lorenzo-Trueba, Low-
resource expressive text-to-speech using data augmentation, in ICASSP 2021-2021 IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,
2021, pp. 6593–6597.
[20] R. Shah, K. Pokora, A. Ezzerg, V. Klimkov, G. Huybrechts, B. Putrycz, D. Korzekwa, and
T. Merritt, Non-Autoregressive TTS with Explicit Duration Modelling for Low-Resource
Highly Expressive Speech, in Proc. 11th ISCA Speech Synthesis Workshop (SSW 11). 2021,
pp. 96–101.
[21] A. Fazel, W. Yang, Y. Liu, R. Barra-Chicote, Y. Meng, R. Maas, and J. Droppo, Syn-
thASR: Unlocking Synthetic Data for Speech Recognition, in Proc. Interspeech 2021. 2021,
pp. 896–900.
[22] D. Korzekwa, J. Lorenzo-Trueba, T. Drugman, S. Calamaro, and B. Kostek, Weakly-
Supervised Word-Level Pronunciation Error Detection in Non-Native English Speech, in
Proc. Interspeech 2021. 2021, pp. 4408–4412.
[23] D. Korzekwa, R. Barra-Chicote, S. Zaporowski, G. Beringer, J. Lorenzo-Trueba, A. Ser-
aﬁnowicz, J. Droppo, T. Drugman, and B. Kostek, Detection of Lexical Stress Errors in
Non-Native (L2) English with Data Augmentation and Attention, in Proc. Interspeech 2021. 2021, pp. 3915–3919.
[24] K. Li, X. Qian, and H. Meng, Mispronunciation detection and diagnosis in l2 english
speech using multidistribution deep neural networks, IEEE/ACM Transactions on Audio,
Speech, and Language Processing 25 (2016), pp. 193–207.
[25] S. Sudhakara, M.K. Ramanathi, C. Yarra, and P.K. Ghosh, An Improved Goodness of
Pronunciation (GoP) Measure for Pronunciation Evaluation with DNN-HMM System
Considering HMM Transition Probabilities., in INTERSPEECH. 2019, pp. 954–958.
[26] S. Cheng, Z. Liu, L. Li, Z. Tang, D. Wang, and T.F. Zheng, Asr-free pronunciation
assessment, arXiv preprint arXiv:2005.11902 (2020).
[27] H. Franco, L. Neumeyer, Y. Kim, and O. Ronen, Automatic pronunciation scoring for language instruction, in 1997 IEEE international conference on acoustics, speech, and
signal processing, Vol. 2. IEEE, 1997, pp. 1471–1474.
[28] N. Minematsu, Pronunciation assessment based upon the phonological distortions observed
in language learners’ utterances, in INTERSPEECH. 2004.
[29] A.M. Harrison, W.K. Lo, X.j. Qian, and H. Meng, Implementation of an extended recog-
nition network for mispronunciation detection and diagnosis in computer-assisted pro-
nunciation training, in Intl. Workshop on Speech and Language Technology in Education.
2009.
[30] A. Lee and J.R. Glass, Pronunciation assessment via a comparison-based system, in
SLaTE. 2013.
[31] P. Plantinga and E. Fosler-Lussier, Towards Real-Time Mispronunciation Detection in
Kids’ Speech, in 2019 IEEE Automatic Speech Recognition and Understanding Workshop
(ASRU). IEEE, 2019, pp. 690–696.
[32] S. Sudhakara, M.K. Ramanathi, C. Yarra, A. Das, and P. Ghosh, Noise robust goodness
of pronunciation measures using teacher’s utterance, in SLaTE. 2019.
[33] L. Zhang, Z. Zhao, C. Ma, L. Shan, H. Sun, L. Jiang, S. Deng, and C. Gao, End-to-end
automatic pronunciation error detection based on improved hybrid ctc/attention architec-
ture, Sensors 20 (2020), p. 1809.
[34] J. Chorowski, D. Bahdanau, K. Cho, and Y. Bengio, End-to-end continuous speech recog-
nition using attention-based recurrent nn: First results, arXiv preprint arXiv:1412.1602
(2014).
[35] J.K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, Attention-based models
for speech recognition, in Advances in neural information processing systems. 2015, pp.
577–585.
[36] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio, End-to-end attention-
based large vocabulary speech recognition, in 2016 IEEE international conference on acous-
tics, speech and signal processing (ICASSP). IEEE, 2016, pp. 4945–4949.
[37] Y. Xiao, F. Soong, and W. Hu, Paired Phone-Posteriors Approach to ESL Pronunciation
Quality Assessment, in Proc. Interspeech 2018. 2018, pp. 1631–1635.
[38] M. Nicolao, A.V. Beeston, and T. Hain, Automatic assessment of English learner pro-
nunciation using discriminative classiﬁers, in 2015 IEEE Intl. Conference on Acoustics,
Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5351–5355.
[39] J. Wang, Y. Qin, Z. Peng, and T. Lee, Child Speech Disorder Detection with Siamese
Recurrent Network Using Speech Attribute Features., in INTERSPEECH. 2019, pp. 3885–
3889.
[40] X. Qian, H. Meng, and F. Soong, Capturing L2 segmental mispronunciations with joint-
sequence models in computer-aided pronunciation training (CAPT), in 2010 7th Intl. Sym-
posium on Chinese Spoken Language Processing. IEEE, 2010, pp. 84–88.
[41] S.B. Needleman and C.D. Wunsch, A general method applicable to the search for similar-
ities in the amino acid sequence of two proteins, Journal of molecular biology 48 (1970),
pp. 443–453.
[42] R. Duan, T. Kawahara, M. Dantsuji, and H. Nanjo, Cross-lingual transfer learning of non-
native acoustic modeling for pronunciation error detection and diagnosis, IEEE/ACM
Transactions on Audio, Speech, and Language Processing 28 (2019), pp. 391–401.
[43] B.C. Yan, M.C. Wu, H.T. Hung, and B. Chen, An End-to-End Mispronunciation Detection
System for L2 English Speech Leveraging Novel Anti-Phone Modeling, in Proc. Interspeech
2020. 2020, pp. 3032–3036.
[44] X. Xu, Y. Kang, S. Cao, B. Lin, and L. Ma, Explore wav2vec 2.0 for Mispronunciation
Detection, in Proc. Interspeech 2021. 2021, pp. 4428–4432.
[45] L. Peng, K. Fu, B. Lin, D. Ke, and J. Zhan, A Study on Fine-Tuning wav2vec2.0 Model for
the Task of Mispronunciation Detection and Diagnosis, in Proc. Interspeech 2021. 2021,
pp. 4448–4452.
[46] B. Lin and L. Wang, Deep Feature Transfer Learning for Automatic Pronunciation As-
sessment, in Proc. Interspeech 2021. 2021, pp. 4438–4442.
[47] J. Field, Intelligibility and the listener: The role of lexical stress, TESOL quarterly 39
(2005), pp. 399–423.
[48] A. Lepage and M.G. Bus`a, Intelligibility of English L2: The eﬀects of incorrect word stress
placement and incorrect vowel reduction in the speech of French and Italian learners of
English, in Proc. of the Intl. Symposium on the Acquisition of Second Language Speech.
2014, 2014, pp. 387–400.
[49] Y.J. Jung, S.C. Rhee, et al., Acoustic analysis of english lexical stress produced by korean,
japanese and taiwanese-chinese speakers, Phonetics and Speech Sciences 10 (2018), pp.
15–22.
[50] D.R.v. Bergem, Acoustic and lexical vowel reduction, in Phonetics and Phonology of Speak-
ing Styles. 1991.
[51] K. Li, S. Mao, X. Li, Z. Wu, and H. Meng, Automatic lexical stress and pitch accent
detection for l2 english speech using multi-distribution deep neural networks, Speech Com-
munication 96 (2018), pp. 28–36.
[52] J. Zhao, H. Yuan, J. Liu, and S. Xia, Automatic lexical stress detection using acoustic
features for computer assisted language learning, Proc. APSIPA ASC (2011), pp. 247–251.
[53] N. Chen and Q. He, Using nonlinear features in automatic English lexical stress detection,
in 2007 Intl. Conference on Computational Intelligence and Security Workshops (CISW
2007). IEEE, 2007, pp. 328–332.
[54] K. Li, X. Qian, S. Kang, and H. Meng, Lexical stress detection for L2 English speech using
deep belief networks., in Interspeech. 2013, pp. 1811–1815.
[55] M.K. Ramanathi, C. Yarra, and P.K. Ghosh, ASR Inspired Syllable Stress Detection
for Pronunciation Evaluation Without Using a Supervised Classiﬁer and Syllable Level
Features., in INTERSPEECH. 2019, pp. 924–928.
[56] J. Badenhorst and F. De Wet, The limitations of data perturbation for ASR of learner data
in under-resourced languages, in 2017 Pattern Recognition Association of South Africa and
Robotics and Mechatronics (PRASA-RobMech). IEEE, 2017, pp. 44–49.
[57] V.V. Eklund, Data augmentation techniques for robust audio analysis, Master’s thesis,
Tampere University, 2019.
[58] A. Lee, et al., Language-independent methods for computer-assisted pronunciation train-
ing, Ph.D. diss., Massachusetts Institute of Technology, 2016.
[59] S. Komatsu and M. Sasayama, Speech Error Detection depending on Linguistic Units,
in Proceedings of the 2019 3rd International Conference on Natural Language Processing
and Information Retrieval. 2019, pp. 75–79.
[60] K. Fu, J. Lin, D. Ke, Y. Xie, J. Zhang, and B. Lin, A full text-dependent end to end
mispronunciation detection and diagnosis with easy data augmentation techniques, arXiv
preprint arXiv:2104.08428 (2021).
[61] B.C. Yan, S.W.F. Jiang, F.A. Chao, and B. Chen, Maximum f1-score training for end-
to-end mispronunciation detection and diagnosis of l2 english speech, arXiv preprint
arXiv:2108.13816 (2021).
[62] G. Huang, A. Gorin, J.L. Gauvain, and L. Lamel, Machine translation based data aug-
mentation for cantonese keyword spotting, in 2016 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 6020–6024.
[63] D. Koller and N. Friedman, Probabilistic graphical models: principles and techniques, MIT
press, 2009.
[64] J. Latorre, J. Lachowicz, J. Lorenzo-Trueba, T. Merritt, T. Drugman, S. Ronanki, and
V. Klimkov, Eﬀect of data reduction on sequence-to-sequence neural tts, in ICASSP
2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). IEEE, 2019, pp. 7075–7079.
[65] A. Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals, K. Kavukcuoglu, G. Driessche, E.
Lockhart, L. Cobo, F. Stimberg, et al., Parallel wavenet: Fast high-ﬁdelity speech synthesis,
in International conference on machine learning. PMLR, 2018, pp. 3918–3926.
[66] Y. Jiao, A. Gabry´s, G. Tinchev, B. Putrycz, D. Korzekwa, and V. Klimkov, Universal neu-
ral vocoding with parallel wavenet, in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6044–6048.
[67] M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, and L.K. Saul, An introduction to variational
methods for graphical models, Machine learning 37 (1999), pp. 183–233.
[68] J.S. Garofolo, L.F. Lamel, W.M. Fisher, J.G. Fiscus, and D.S. Pallett, Darpa timit
acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1, STIN 93 (1993),
p. 27403.
[69] H. Zen, V. Dang, R. Clark, Y. Zhang, R.J. Weiss, Y. Jia, Z. Chen, and Y. Wu, LibriTTS:
A Corpus Derived from LibriSpeech for Text-to-Speech, in Proc. Interspeech 2019. 2019,
pp. 1526–1530.
[70] E. Atwell, P. Howarth, and D. Souter, The isle corpus: Italian and german spoken learner’s
english, ICAME Journal: Intl. Computer Archive of Modern and Medieval English Journal
27 (2003), pp. 5–18.
[71] D. Weber, S. Zaporowski, and D. Korzekwa, Constructing a Dataset of Speech Recordings
with Lombard Eﬀect, in 24th IEEE SPA. 2020.
[72] J. Kominek and A.W. Black, The CMU Arctic speech databases, in Fifth ISCA workshop
on speech synthesis. 2004.
[73] A. Porzuczek and A. Rojczyk, English word stress in polish learners speech production
and metacompetence, Research in Language 15 (2017), pp. 313–323.
[74] A. Graves, Connectionist temporal classiﬁcation, in Supervised Sequence Labelling with
Recurrent Neural Networks, Springer, 2012, pp. 61–93.
[75] I. Sutskever, O. Vinyals, and Q.V. Le, Sequence to sequence learning with neural networks,
in Advances in neural information processing systems. 2014, pp. 3104–3112.
[76] A.E. Hieke, Linking as a marker of ﬂuent speech, Language and Speech 27 (1984), pp.
343–354.
[77] J.B. Michel, Y.K. Shen, A.P. Aiden, A. Veres, M.K. Gray, J.P. Pickett, D. Hoiberg, D.
Clancy, P. Norvig, J. Orwant, et al., Quantitative analysis of culture using millions of
digitized books, science 331 (2011), pp. 176–182.
[78] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N. Gomez, L. Kaiser, and
I. Polosukhin, Attention is all you need, in Advances in neural information processing
systems. 2017, pp. 5998–6008.
[79] T. Merritt, A. Ezzerg, P. Bili´nski, M. Proszewska, K. Pokora, R. Barra-Chicote, and D.
Korzekwa, Text-free non-parallel many-to-many voice conversion using normalising ﬂows,
Accepted to Acoustics, Speech and Signal Processing (ICASSP) (2022).
