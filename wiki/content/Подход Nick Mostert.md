## Небольшое pre

1. Используется 2 датасета - Malmberg dataset, Auris dataset
2. Malmberg = samples + detailed_annotations
3. Auris = one_word_sample
4. MDD - mispronunciation detection and diagnosis
5. Задача MDD - выявлять неправильное сегментирование произношения и предоставлять диагностику на уровне фонем пользователю

## Nick Mostert описывает ситуацию в MDD

### Самый распространенный подход к MDD

==**Cквозная фонетическая оценка**== (end-to-end phonetic transcription) - самый распространенный подход, описан в следующих статьях:

[44] L. Peng, K. Fu, B. Lin, D. Ke, and J. Zhang. A study on fine-tuning wav2vec2. 0 model for the task of mispronunciation detection and diagnosis. In Interspeech, pages 4448–4452, 2021.
[62] M. Wu, K. Li, W.-K. Leung, and H. Meng. Transformer based end-to-end mispronunciation detection and diagnosis. In Interspeech, pages 3954–3958, 2021.
[66] M. Yang, K. Hirschi, S. D. Looney, O. Kang, and J. H. Hansen. Improving mispronunciation detection with Wav2vec2-based momentum pseudo-labeling for accentedness and intelligibility assessment. arXiv preprint arXiv:2203.15937, 2022.

Такие модели не только указывают, была ли допущена ошибка, но и точно показывает, какой звук был неверно произнесен и в каком месте слова.

Этот метод эффективно использует модели, предварительно обученные на неразмеченных данных, таких как Wav2Vec 2.0, что частично компенсирует нехватку аннотированных данных.

Архитектура:
- **Предобученная Wav2Vec 2.0 дообучается**, чтобы назначать каждому моменту времени либо звук, либо пустой символ.
	Каждой части аудиосигнала ставится в соответствии какой-то звук или его отсутствие (пауза/или просто тишина).

Wav2Vec берет звук и преобразует в последовательность эмбеддингов из акустических признаков.
	Например: сказали голосом "cat", получили от Wav2Vec - vec1, vec2, vec3

- **CTC** (Connectionist Temporal Classification - связная временнАя классификация) преобразует эту последовательность в фонетическую транскрипцию.
	Например, CTC получил:  vec1, vec2, vec3
	CTC преобразовал векторы в [k], [ae], [t]
	
- Полученная от CTC транскрипция сравнивается с канонической транскрипцией слова, чтобы определить, была ли ошибка.


1. [62] сравнивает модель Wav2Vec 2.0 с тремя другими моделями (стандартная модель трансформера, модель CNN-RNN-CTC и модель перехода состояний звуков - phone state transition model) 
	Их модель превзошла базовые модели с F-мерой 80,98%. Лучшая из базовых моделей была модель ==CNN-RNN-CTC== с F-мерой 74,62%. ==Лучшие на данный момент== 

2. [44] и [66] использовали аналогичную модель, дообученную на корпусе L2-ARCTIC.
- [44] использовал более крупную многоязычную версию Wav2Vec 2.0. Достигли лучших предсказаний с F-мерой 59,73%.
- [66] использовал базовую модель, комбинированную с алгоритмом псевдоразметки для улучшения их обучающего набора данных 
	(Этот алгоритм использует учителя-модель для предсказания псевдоразметок для неразмеченных данных. Эти псевдоразмеченные данные затем используются для дообучения студента-модели. Наконец, веса учителя обновляются как среднее между старой моделью учителя и дообученной моделью студента. И учитель, и студент изначально инициализируются как одна и та же дообученная модель Wav2Vec 2.0.) 
	
	Кроме того, они проверяли, была ли корреляция между уровнем ошибок в звуках (PER), предсказанным их моделью, и человеческим восприятием понятности и акцентированности. 
	
	Получили F-меру 56,16%, что было небольшим улучшением по сравнению с их базовой моделью, не использующей псевдоразметку.
	
	Выяснили, что их модель может предсказывать понятность и акцентированность высказывания с коэффициентами корреляции -0,84 и -0,75 соответственно. Это свидетельствует о том, что акцент студента и общая понятность высказывания играют большую роль в эффективности классификации звуков для изучающих второй язык, хотя это может быть частично связано с тем, что их модель Wav2Vec 2.0 была в основном предварительно обучена на носителях английского языка.


### Другой подход - предсказывание ошибок напрямую

[65] X. Xu, Y. Kang, S. Cao, B. Lin, and L. Ma. Explore Wav2Vec 2.0 for mispronunciation detection. In Interspeech, pages 4428–4432, 2021.

[65] предложили другой подход к корпусу L2-ARCTIC. Вместо использования сквозной фонологической транскрипции они пытаются предсказывать ошибки произношения напрямую. 

Сначала они используют HMM-DNN для определения диапазона выравнивания (l, r) каждого звука в целевом предложении. 

Затем применяется покадровая свертка на каждом временном шаге в этом диапазоне, а слой максимального адаптивного пулинга выбирает наибольший результат. 

Окончательное предсказание выполняется с помощью сигмоидной функции. 

Попытка произношения классифицируется как правильная, если каждый звук в каноническом произношении целевого слова классифицируется как правильный. 

Их метод улучшил результаты [66] и [44] с F-мерой 61,0%. 

### Другие языки
Наиболее распространённые версии Wav2Vec 2.0 предобучены либо на корпусе Librispeech (LS-960), который содержит 960 часов аудио, либо на корпусе LibriVox (LV-60K), который содержит примерно 53 тысячи часов аудио после предварительной обработки. Оба этих корпуса содержат только аудиозаписи на английском языке. Если взять одну из этих моделей и дообучить её для автоматического распознавания речи (ASR) на данных с разметкой на другом языке, то производительность модели значительно ухудшится по сравнению с английской версией. То же самое происходит с классификацией звуков.

==Librispeech - начитка аудиокниг на англйиском носителями-добровольцами, без ошибок. Имеется только аудио и текст. Аналогичный датасет - LibriVox, но он больше.==
==Mozilla Common Voices - содержит ошибки, но они не маркируются отдельно. Multilingual.== 
BABEL -  нет ошибок в произношении.
Multilingual LibriSpeech (MLS) - нет ошибок в произношении.


1. **LibriSpeech** (возможно пересечение с LibriVox)
- **Тип данных**: Начитка аудиокниг на английском языке носителями-добровольцами.
- **Язык**: eng
- **Объем**: 960 ч
- **Содержимое**: Содержит **аудио** и **текстовые представления** (расшифровки).
- **Ошибки произношения**: **Ошибок в произношении нет**; записи выполняются носителями языка, что обеспечивает высокое качество.
2. **LibriVox** (возможно пересечение с LibriSpeech)
- **Тип данных**: Аудиокниги, начитанные волонтерами.
- **Язык**:  multi
- **Объем**: 53 000 ч
- **Содержимое**: Содержит **аудио** и **текстовые представления**.
- **Ошибки произношения**: **Ошибок в произношении нет**, но записи могут варьироваться по качеству из-за разных дикторов.
3. **Mozilla CV** 
- **Тип данных**: Мультиязычные записи.
- **Язык**: multi
- **Объем**: 53 000 ч
- **Содержимое**: Содержит **аудио** и **текстовые представления**.
- **Ошибки произношения**: Содержит **ошибки произношения**, но они **не маркируются отдельно**.
4. **Babel**
- **Тип данных**: Записи разговорной речи на различных языках.
- **Язык**: multi
- **Объем**: 50 000 ч
- **Содержимое**: Содержит **аудио** и **текстовые представления**.
- **Ошибки произношения**: **Ошибок в произношении нет**; записи сделаны в рамках контролируемых условий, чтобы минимизировать ошибки.
5. **Multilingual LibriSpeech** 
- **Тип данных**: Мультиязычные записи аудиокниг.
- **Язык**: multi
- **Объем**: 1000 ч
- **Содержимое**: Содержит **аудио** и **текстовые представления**.
- **Ошибки произношения**: **Ошибок в произношении нет**; аудио записаны носителями языка.

Наконец, была предобучена большая модель на всём корпусе CV, а также на корпусах BABEL и Multilingual LibriSpeech (MLS), что составило в общей сложности 56 тысяч часов аудио на 53 разных языках. Эта модель называется **XLSR-53**, и она значительно превзошла все остальные модели по всем исследуемым языкам в задаче классификации звуков.



## Nick Mostert описывает PRAAT  [1]

### Датасет CHOREC

- **Корпус устного чтения детей (CHOREC)** 
	Cобрана информация о поле, возрасте, классе, учебной программе, месте рождения, месте проживания, родном языке, уровне чтения, методе, по которому дети учились читать, и наличии нарушений чтения.
	https://hdl.handle.net/10032/tm-a2-j5
	Детское чтение нидерландского - слова, псевдослова, истории

==Корпус был аннотирован ВРУЧНУЮ с помощью инструмента Praat. ==
Praat позволяет создавать несколько уровней, которые могут быть использованы для различных аннотаций, привязанных ко времени. 

В CHOREC использовались следующие 8 уровней аннотаций:

- Подали в Praat аудио
- Для разметки аудиофайла создается несколько уровней аннотации:
	1. Ожидаемый текст
		заносится текст (вручную)
	2. Ожидаемый текст с корректировка границ
		аннотатор разбивает текст на фразы, слова, паузы, ...
	3. Орфографическая транскрипция сказанного - Ortography
		Слова с ошибками произношения отмечены символом «*». Если произношение похоже на другое нидерландское слово, это слово добавляется в скобках; в противном случае используется отметка «*s».
	4. Широкая фонетическая транскрипция - Phonetics
		Ошибочные произношения также отмечены «* ».
		Фактическая транскрипция
	5. Аннотация высказываний экзаменатора
		Если экзаменатор попал в аудио
	6.  Аннотация шума
		Если шум попал в аудио
	7. Стратегия чтения - Strategy
		Если были допущены ошибки при чтении, аннотируется стратегия чтения. Возможные аннотации: «неправильно на первом повторении», «правильно после нескольких неслышных попыток», «правильно на первом повторении», «повторение начальной части слова» и «пропуск слова».
	8. Аннотация ошибки чтения
		Для слов, в которых последняя попытка была ошибочной, аннотируется тип ошибки из классификационной системы, содержащей 40 категорий ошибок чтения.

Пример аннотированного предложения можно увидеть на рисунке:
![[Pasted image 20241005230032.png]]

Аннотации уровней 5 и 6 опущены, поскольку в данном примере не было фонового шума или перекрёстных разговоров.

### Датасет Malmberg

Этот датасет содержит 2,5 тысячи записей детей, читающих короткие рассказы вслух.

Данные также аннотированы с помощью программы Praat. 
Существует 4 уровня аннотаций, каждый с соответствующими временными интервалами:

1. **Words (Слова)**: слова из текста, которые были правильно произнесены.
    
2. **OutOfPrompt (Вне текста)**: аннотации высказываний, которые не относятся к исходному тексту. Возможные аннотации:
    
    - Substitution (заменённое слово),
    - InsertionArticle (артикль, который не был в тексте),
    - InsertionUtterance (несколько вставленных слов),
    - InsertionSounds (вставленные звуки, которые не являются словами),
    - InsertionWord (вставлено слово, которое не является артиклем),
    - Omission (слово из текста пропущено).
    
    Все аннотации, кроме пропуска, ортографически обозначены.
    
3. **Errors (Ошибки)**: аннотации различных типов ошибок. Возможные аннотации:
    
    - ProlongedVowels - ПротяжённыеГласные,
    - Hesitation - Колебание,
    - DifficultIntelligibility - ТруднаяРазборчивость,
    - PhonemicChange - ФонемическоеИзменение,
    - FalseStart - ЛожноеНачало,
    - RepetitionWord - ПовторСлова,
    - Decoding - Декодирование,
    - SubstitutionSequence - ЗаменаПоследовательности,
    - WordStress - УдарениеСлова,
    - SlipOfTheTongue - Оговорка,
    - SlipOfTheTongueCorrected - ИсправленнаяОговорка,
    - PauseTooLong - СлишкомДолгаяПауза,
    - SubstitutionSemantic - СемантическаяЗамена,
    - CutShortOrInterruptedSlipOfTheTongue - ПрерваннаяОговорка,
    - Pauses - Паузы,
    - IncomprehensibleUtterances - НеразборчивыеВысказывания.
    
    Ошибки и их исправления, где возможно, аннотируются ортографически, а также включаются попытки произнести слово из текста, как в случае с FalseStart.
    
4. **Noise (Шум)**: аннотации шумов, не связанных с чтением текста. Возможные аннотации:
    
    - NoSignal - ОтсутствиеСигнала,
    - DistortedAudio - ИскажённоеАудио,
    - LowVolume - НизкийУровеньГромкости,
    - BackgroundNoise - ФоновыйШум,
    - CrossTalk - ПерекрёстныйРазговор,
    - SpeechOthers - РечьДругихЛюдей.

- Примером текста было предложение: «Maar de pop is en blijft weg.» (Но кукла исчезла и не вернулась). 
- Студент колебался и начал повторяться, читая предложение, в результате чего прозвучало: «Maar de pop pop is de pop is en bl blijft weg». 
- Следующие метки сокращены в примере: ВставкаАртикля (IA), ВставкаСлова (IW), ВставкаЗвука (IS), ПовторСлова (RW), ЛожноеНачало (FS). 
- Уровень шумов в этом примере был пустым и поэтому пропущен. 
- Для ошибок на уровне OutOfPrompt, таких как ВставкаСлова, вставленное слово отображается после «|». 
- Для некоторых ошибок на уровне Error, таких как ЛожноеНачало, слово, которое студент ошибочно пытался произнести, отображается после «|>», а также временная метка правильного произнесения этого слова (не показано в примере).
![[Pasted image 20241005233428.png]]

## Как моделировал

- Использовал только CHOREC
- Две модели Wav2Vec 2.0. 

- Первая будет использовать алгоритм CTC для классификации звуков (PC) и генерации фонетической транскрипции высказывания. 
- Эта транскрипция затем может быть использована для определения, был ли целевой звук произнесен правильно. Этот подход аналогичен стандартным моделям для преобразования речи в текст (ASR). 

- Вторая модель будет использовать контекстно-зависимый вывод модели Wav2Vec 2.0 для прямой классификации (DC) того, был ли целевой звук произнесен.

- Помимо этих моделей я также реализую базовую модель, основанную на транскрипционно-глобальной оценке произношения (trans-GOP).
## Предобработка данных

 - Набор данных будет включать только задания с произношением списка слов и псевдослов, но не задание по чтению текста. 
 - Все аудиофайлы со списком слов будут разделены на попытки произнесения отдельных слов. 
- Все аудио можно очистить, удалив сегменты, аннотированные как шум. 
- Аудио с перечнем слов можно разделить на образцы, состоящие из отдельных слов. 
- Эти данные затем можно отсортировать и обработать партиями схожей длины, чтобы минимизировать количество заполнений (padding).

## P.S.
В работах  используется языковая модель на основе n-грамм (LM) для классификации звуков в некоторых наборах данных. LM учитывает предыдущие n−1 звуков при предсказании следующего звука. Важно отметить, что в обоих исследованиях использовались образцы чтения текстов с относительно небольшим количеством ошибок произношения. В таких образцах LM может уловить общие паттерны произношения и последовательности звуков языка. Я считаю, что использование LM в данной задаче менее оправдано, так как пациенты, проходящие терапию речи, могут не следовать общепринятым паттернам произношения, а LM может сделать модель более предвзятой в сторону правильных произношений. 

Окончательная последовательность звуков предсказывается с помощью декодирования методом луча с размером луча 50, как это принято в [6], [19] и [64]. Эта последовательность звуков затем сравнивается с аннотированной фонетической транскрипцией для оценки производительности модели. Оптимальные гиперпараметры будут выбраны на основе показателя ошибок звуков (PER) на валидационных наборах данных.